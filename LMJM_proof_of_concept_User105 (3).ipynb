{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1W-Lrtw3zDL"
   },
   "source": [
    "## Import all packages that will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jm1rfydd3yUb"
   },
   "outputs": [],
   "source": [
    "# Numpy for data management\n",
    "import numpy as np\n",
    "\n",
    "# Pandas also for data management\n",
    "import pandas as pd\n",
    "\n",
    "# Matplotlib for additional customization\n",
    "#import pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#For uploading our dataset\n",
    "import xml.etree.ElementTree as ET #Parse and read XML data\n",
    "import tarfile #read from tarfile instead of extracting all data\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import trec\n",
    "import pprint as pp\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding LMJM algorithm (with example datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For references to the examples please see: https://nlp.stanford.edu/IR-book/pdf/12lmodel.pdf and https://www.elastic.co/blog/language-models-in-elasticsearch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the formula for Jelinek Mercer smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(q | d) =\\prod_{t \\in q} ((1 - \\lambda) \\frac{tf_{t,d}}{L_d} + \\lambda \\frac{tf_t}{L_c})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where, $\\lambda$ is from 0 to 1 is our smoothing parameter. tf_{t,d} is a term frequency in our document and tf_t is a term frequency in corpus. L_d is length of tokens in doc, L_c is length of tokens in all document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the formula for Jelinek Mercer smoothing with logs as used in elastic search algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(q| d) = \\sum_{t \\in q} \\log(1 + \\frac{(1- \\lambda) M_d}{\\lambda M_c}) = \\sum_{t \\in q} \\log(1 + \\frac{(1- \\lambda) \\frac{tf_{t,d}}{L_d}}{\\lambda \\frac{tf_t + 1}{L_c + 1}}) $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00413321 0.0036624  0.00330748]\n"
     ]
    }
   ],
   "source": [
    "corpus_4 = [\n",
    "     'This is the desert There are no people in the desert The Earth is large',\n",
    "     'Where are the people? resumed the little prince at last Its a little lonely in the desert It is lonely when youre among people too said the snake',\n",
    "'What makes the desert beautiful said the little prince is that somewhere it hides a well',]\n",
    "\n",
    "index_test = CountVectorizer(ngram_range=(1,1), analyzer='word', stop_words = None)\n",
    "index_test.fit(corpus_4)\n",
    "\n",
    "corpus_cv = index_test.transform(corpus_4)\n",
    "corpus_array=corpus_cv.toarray()\n",
    "\n",
    "df_corpus = pd.DataFrame(data=corpus_array,columns = index_test.get_feature_names_out())\n",
    "\n",
    "#print(df_corpus)\n",
    "\n",
    "l=0.1\n",
    "\n",
    "# Compute the query representation \n",
    "\n",
    "query = ['desert people']\n",
    "\n",
    "query_cv = index_test.transform(query)\n",
    "#print(query_cv.todense())\n",
    "qq = query_cv.toarray()[0]\n",
    "#print(qq)\n",
    "A=len(Counter(corpus_4))  \n",
    "aa = np.tile(qq, [A,1]) \n",
    "#print(\"aa \", aa)\n",
    "#print(\"corpos array \",corpus_array.T)\n",
    "prob_word_docs = corpus_array.T/np.sum(corpus_array,axis=1) # divided by doclength\n",
    "#print(\"prob word docs \", prob_word_docs)\n",
    "prob_word_corpus = np.sum(corpus_array, axis=0)/np.sum(corpus_array)\n",
    "prob_word_docs_query =(1-l)*(prob_word_docs.T**aa)\n",
    "prob_word_corpus_query = l*(prob_word_corpus**aa)\n",
    "docs_scores = prob_word_docs_query + prob_word_corpus_query\n",
    "final = np.prod(docs_scores, axis = 1)\n",
    "\n",
    "\n",
    "print(final)\n",
    "\n",
    "#print(np.sum(corpus_array,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01171875 0.00390625]\n"
     ]
    }
   ],
   "source": [
    "corpus_3 = [\n",
    "     'Xyzzy this reports profit but revenue is down',\n",
    "     'Quorus narrows quarter loss but revenue increases further',]\n",
    "\n",
    "index_test = CountVectorizer(ngram_range=(1,1), analyzer='word', stop_words = None)\n",
    "index_test.fit(corpus_3)\n",
    "\n",
    "corpus_cv = index_test.transform(corpus_3)\n",
    "\n",
    "corpus_array=corpus_cv.toarray()\n",
    "\n",
    "df_corpus = pd.DataFrame(data=corpus_array)\n",
    "\n",
    "#print(df_corpus)\n",
    "\n",
    "l=0.5\n",
    "\n",
    "# Compute the query representation \n",
    "\n",
    "query = ['revenue down',]\n",
    "query_cv = index_test.transform(query)\n",
    "\n",
    " \n",
    "qq = query_cv.toarray()[0]\n",
    "A=len(Counter(corpus_3))  \n",
    "aa = np.tile(qq, [A,1]) \n",
    "\n",
    "prob_word_docs = corpus_array.T/np.sum(corpus_array,axis=1) # divided by doclength\n",
    "prob_word_corpus = np.sum(corpus_array, axis=0)/np.sum(corpus_array)\n",
    "prob_word_docs_query =(1-l)*(prob_word_docs.T**aa)\n",
    "prob_word_corpus_query = l*(prob_word_corpus**aa)\n",
    "docs_scores = prob_word_docs_query + prob_word_corpus_query\n",
    "final = np.prod(docs_scores, axis = 1)\n",
    "\n",
    "\n",
    "#prob_word_docs = corpus_array.T/np.sum(corpus_array,axis=1)\n",
    "#prob_word_corpus = np.sum(corpus_array, axis=0)/np.sum(corpus_array)\n",
    "#joint_probabilities = np.multiply(prob_word_docs.T, aa)*(1-l) + np.multiply(prob_word_corpus, aa)*l\n",
    "#doc_scores_lmjm = np.sum(joint_probabilities, axis=1) \n",
    "\n",
    "\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RI_project_part1X.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
